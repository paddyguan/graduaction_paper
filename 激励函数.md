# 激励函数

### 定义

激活函数是一种数学形式，用来表现近似于细胞对轴突或神经元的影响。神经网络中的每个节点接受输入值，并将输入值传递给下一层，输入节点会将输入属性值直接传递给下一层（隐层或输出层）。在神经网络中，隐层和输出层节点的输入和输出之间具有函数关系，这个函数称为激励函数，也称为传递函数。

### 作用

没有激励函数的情况下神经网络的每层都只是做线性变换，多层输入叠加后依旧是线性变换。由于线性模型的表达能力不够，要引入非线性因素的激励函数。

如果没有激励函数，在单层神经网络中，我们的输入和输出关系如下图：



![这里写图片描述](http://img.blog.csdn.net/20160630103416973)

这是一个线性方程的直观表示，如果我们增加感知机，如下图：

![这里写图片描述](http://img.blog.csdn.net/20160630104959185)

**其中结果输出就是右边的绿色区域，但是本质上还是各种线性方程的组合，对于非线性数据则无法很好的处理。**



![聊一聊深度学习的activation function](https://pic4.zhimg.com/v2-61fe81589ab491d1d3ba612b3bdf5b51_1200x500.jpg)



### 类型

常见的激励函数介绍

1. sigmoid函数

   公式：$ f(x) = {1 \over 1 + e^{-x}}$

   sigmoid函数输入一个实值的数，然后将其压缩到0~1的范围内。特别地，大的负数被映射成0，大的正数被映射成1。sigmoid function在历史上流行过一段时间因为它能够很好的表达“激活”的意思，未激活就是0，完全饱和的激活则是1。

   缺点：

   * Sigmoid容易饱和，即所有数都接近最大值1。当输入非常大或者非常小的时候，神经元的梯度就接近于0，这就使得我们在反向传播算法中反向传播接近于0的梯度，导致最终权重基本没什么更新而无法二归地学习到输入数据。另外，尤其注意参数的初始值来尽量避免饱和的情况。如果输入的初始值很大的话，大部分神经元可能都会处在饱和的状态导致梯度爆炸问题，这会导致网络变的很难学习。
   * Sigmoid 的输出不是0均值的，这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。假设后层神经元的输入都为正(e.g. x>0 elementwise in )，那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 如果是按batch去训练，那么每个batch可能得到不同的符号（正或负），那么相加之后这个问题可以缓解。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度爆炸问题相比好处理一点。

2. tanh 函数

公式：$f(x) = {sinh(x) \over cosh(x)} = {e^x - e^{-x} \over e^x + e^{-x}}$

与 sigmoid 不同的是，tanh 是0均值的。tanh是双曲正切函数，tanh函数和sigmod函数的曲线是比较相近的，咱们来比较一下看看。首先相同的是，这两个函数在输入很大或是很小的时候，输出都几乎平滑，梯度很小，不利于权重更新；不同的是输出区间，tanh的输出区间是在(-1,1)之间，而且整个函数是以0为中心的，这个特点比sigmod的好。

一般二分类问题中，隐藏层用tanh函数，输出层用sigmod函数。不过这些也都不是一成不变的，具体使用什么激活函数，还是要根据具体的问题来具体分析，还是要靠调试的。

3. ReLu 函数

   公式：$f(x) = max(0, x)$

   输入信号 \<0时，输出为0；\>0时，输出等于输入。

   优点：

   * 使用 ReLU 得到的SGD的收敛速度会比 sigmoid/tanh 快很多。因为它是linear，所以梯度不会饱和
   * 相比于 sigmoid/tanh需要计算指数等，计算复杂度高，ReLU 只需要一个阈值就可以得到激活值。

   缺点

   * ReLU在训练的时候很”脆弱”，一不小心有可能导致神经元”坏死”。举个例子：由于ReLU在x<0时梯度为0，这样就导致负的梯度在这个ReLU被置零，而且这个神经元有可能再也不会被任何数据激活。如果这个情况发生了，那么这个神经元之后的梯度就永远是0了，也就是ReLU神经元坏死了，不再对任何数据有所响应。实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都坏死了。 当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。

4. ReLUs函数

   公式： $f(x)=1(x<0)$

   $f(x) = (ax)+1(x>=0)(x)$

   Leaky ReLU. Leaky ReLUs 就是用来解决ReLU坏死的问题的。和ReLU不同，当x<0时，它的值不再是0，而是一个较小斜率(如0.01等)的函数。这样，既修正了数据分布，又保留了一些负轴的值，使得负轴信息不会全部丢失。